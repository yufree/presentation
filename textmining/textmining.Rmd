---
title: "Text Mining for Academic Journals"
subtitle: "Miao Yu"
author: "Pawliszyn Research Group"
date: "2017/09/20"
output:
  xaringan::moon_reader:
    css: ["default", "my-theme.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.align='center',echo = F, cache=T,message=FALSE,warning=FALSE,comment=NA)
library(tidyverse)
library(lubridate)
library(stringr)
library(broom)
library(scales)
library(widyr)
library(igraph)
library(ggraph)
library(topicmodels)
library(qrencoder)
```
## How far away between PostDoc and PI?

```{r echo=FALSE, out.width='80%'}
knitr::include_graphics('https://yufree.github.io/presentation/figure/pdpi.png')
```

von Bartheld CS, Houmanfar R, Candido A. (2015)
https://doi.org/10.7717/peerj.1262
---
class: inverse, center, middle

# Get Started

???
Who changed research area as a PDF?
Share my experiences in new research area
---
# Common workflow - SCSU
--

## 1. Search keywords on WOS/Pubmed/Scopus
--

## 2. Collect tons of papers
--

## 3. Start with few reviews
--

## 4. Update with RSS

---
class: inverse, center, middle

# My Story

???

only be focused on one topic, have no idea about the whole domain

PDF should have a larger scope as a Prof? no, editor

---

# Text Mining

## Term Frequency

## Temporal Trends

## Word Co-ocurrences

## Topic modeling

???

Text Mining, or natural language processing, use statistical method to find the rules behind literatures
---

# Data collection

Here I collected the information from all the papers published in *Analytical Chemistry* in the past five years

```{r fetchdata,message=TRUE,echo=TRUE}
# devtools::install_github('yufree/scifetch')
library(scifetch)
query <- '0003-2700[TA] AND 2012/08:2017/08[DP]'
# fetch data
tmdf <- getpubmed(query, start = 1, end = 10000) %>%
        getpubmedtbl() %>%
        mutate(time = as.POSIXct(date, origin = "1970-01-01"),
         month = round_date(date, "month"))
```

???

five year means the trends currently happened while not included in textbook
we found 8676 records

---
# Frequently used words

```{r papers}
wordft <- tmdf %>%
        filter(nchar(title) > 0) %>%
        unnest_tokens(word, title) %>%
        anti_join(stop_words) %>%
        filter(str_detect(word, "[^\\d]")) %>%
        group_by(word) %>%
        mutate(word_total = n()) %>%
        ungroup() %>%
        mutate(source = 'title')

wordfabs <- tmdf %>%
        filter(nchar(abstract) > 0) %>%
        unnest_tokens(word, abstract) %>%
        anti_join(stop_words) %>%
        filter(str_detect(word, "[^\\d]")) %>%
        group_by(word) %>%
        mutate(word_total = n()) %>%
        ungroup() %>%
        mutate(source = 'abstract')

wordf <- rbind.data.frame(wordft[,c('date','time','month','word','word_total','source','line')],wordfabs[,c('date','time','month','word','word_total','source','line')])

```

```{r Frequently used words,fig.width=8}
wordf %>%
        count(word,word_total,sort = T) %>%
        top_n(10, n) %>%
        mutate(word = reorder(word, n)) %>%
        ggplot(aes(word, n)) +
        geom_col(show.legend = FALSE) +
        ylab("Top 10 frequently used words in title/abstract") +
        coord_flip() +
        theme(axis.text=element_text(size=18),
              axis.title=element_text(size=20,face="bold"))
```

---

# Preferred words

```{r wordsource,fig.width=12}
words_by_source <- wordf %>%
        count(source, word, sort = TRUE) %>%
        ungroup()

tf_idf <- words_by_source %>%
        bind_tf_idf(word, source, n) %>%
        arrange(desc(tf_idf))

tf_idf %>%
        group_by(source) %>%
        top_n(10, tf_idf) %>%
        ungroup() %>%
        mutate(word = reorder(word, tf_idf)) %>%
        ggplot(aes(word, tf_idf, fill = source)) +
        geom_col(show.legend = FALSE) +
        facet_wrap( ~ source, scales = "free") +
        ylab("Top 10 preferred words in title/abstract") +
        coord_flip() +
        theme(axis.text = element_text(size = 18),
              axis.title = element_text(size = 20, face = "bold"),
              strip.text.x = element_text(size = 12)
              )
```

---
# Temporal Trends - Growing Words

```{r time,fig.width=12}
papers_per_month <- tmdf %>%
  group_by(month) %>%
  summarize(month_total = n())

word_month_counts <- wordft %>%
  filter(word_total >= 100) %>%
  count(word, month) %>%
  complete(word, month, fill = list(n = 0)) %>%
  inner_join(papers_per_month, by = "month") %>%
  mutate(percent = n / month_total) %>%
  mutate(year = year(month) + yday(month) / 365) %>%
        filter(percent < 0.8)

mod <- ~ glm(cbind(n, month_total - n) ~ year, ., family = "binomial")

slopes <- word_month_counts %>%
  nest(-word) %>%
  mutate(model = map(data, mod)) %>%
  unnest(map(model, tidy)) %>%
  filter(term == "year") %>%
  arrange(desc(estimate))

slopes %>%
  head(9) %>%
  inner_join(word_month_counts, by = "word") %>%
  mutate(word = reorder(word, -estimate)) %>%
  ggplot(aes(month, n / month_total, color = word)) +
  geom_line(show.legend = FALSE) +
  scale_y_continuous(labels = percent_format()) +
  facet_wrap(~ word, scales = "free_y") +
  expand_limits(y = 0) +
  labs(x = "Year",
       y = "Percentage of titles containing this term",
       title = "9 fastest growing words",
       subtitle = "Judged by growth rate over 5 years")+
        theme(axis.text = element_text(size = 18),
              axis.title = element_text(size = 20, face = "bold"),
              strip.text.x = element_text(size = 22)
              )
```
---

# Temporal Trends - Shrinking Words
```{r time2,fig.width=12}
slopes %>%
  tail(9) %>%
  inner_join(word_month_counts, by = "word") %>%
  mutate(word = reorder(word, estimate)) %>%
  ggplot(aes(month, n / month_total, color = word)) +
  geom_line(show.legend = FALSE) +
  scale_y_continuous(labels = percent_format()) +
  facet_wrap(~ word, scales = "free_y") +
  expand_limits(y = 0) +
  labs(x = "Year",
       y = "Percentage of titles containing this term",
       title = "9 fastest shrinking words",
       subtitle = "Judged by growth rate over 5 years")+
        theme(axis.text = element_text(size = 18),
              axis.title = element_text(size = 20, face = "bold"),
              strip.text.x = element_text(size = 22)
              )
```

---
class: inverse, center, middle

# Solid Phase MicroExtraction

---

# From words to terms

```{r SPME}
library(scifetch)
query <- 'Solid Phase MicroExtraction[MH] AND 2007/08:2017/08[DP]'
tmdf <- getpubmed(query, start = 1, end = 10000) %>%
        getpubmedtbl() %>%
        mutate(time = as.POSIXct(date, origin = "1970-01-01"),
         month = round_date(date, "month"))
```
```{r trend,fig.width=12}
papers_per_month <- tmdf %>%
  group_by(month) %>%
  summarize(month_total = n())

wordfabs <- tmdf %>%
        filter(nchar(abstract) > 0) %>%
        unnest_tokens(word, abstract) %>%
        anti_join(stop_words) %>%
        filter(str_detect(word, "[^\\d]")) %>%
        group_by(word) %>%
        mutate(word_total = n()) %>%
        ungroup() 

title_word_pairs <- wordfabs %>%
        pairwise_count(word,line,sort = TRUE)

set.seed(42)
title_word_pairs %>%
  filter(n >= 800) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 2) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  labs(title = "Bigrams in abstract") +
  theme_void()+
        theme(axis.text = element_text(size = 22)
              )
```

---

# Insights in Text Mining

## Mass Spectrum 

## Bioanalytical Chemistry

## Solid Phase MicroExtraction

--

Use Mass Spectrum and Solid Phase MicroExtraction to study Biological problems

---
class: inverse, center, middle

# Metabolomics

???
larger scope, such as Science, Nature
---

# Topic Model

```{r TM}
query <- '(1095-9203[TA] OR 0028-0836[TA]) AND 2014/08:2017/08[DP]'
xml2_1 <- getpubmed(query, start = 1, end = 10000)
xml2_2 <- getpubmed(query, start = 10001, end = 20000)

tmdf1 <- getpubmedtbl(xml2_1)
tmdf2 <- getpubmedtbl(xml2_2)

tmdf <- bind_rows(tmdf1,tmdf2) %>%
        mutate(time = as.POSIXct(date, origin = "1970-01-01"),
         month = round_date(date, "month"))

wordfabs <- tmdf %>%
        filter(nchar(abstract) > 0) %>%
        unnest_tokens(word, abstract) %>%
        anti_join(stop_words) %>%
        filter(str_detect(word, "[^\\d]")) %>%
        group_by(word) %>%
        mutate(word_total = n()) %>%
        ungroup()

wordft <- tmdf %>%
        filter(nchar(title) > 0) %>%
        unnest_tokens(word, title) %>%
        anti_join(stop_words) %>%
        filter(str_detect(word, "[^\\d]")) %>%
        group_by(word) %>%
        mutate(word_total = n()) %>%
        ungroup()
```
```{r TMplot,fig.width=12}
desc_dtm <- wordfabs %>%
        count(line, word, sort = TRUE) %>%
        ungroup() %>%
        cast_dtm(line, word, n)

desc_lda <- LDA(desc_dtm, k = 8, control = list(seed = 42))
tidy_lda <- tidy(desc_lda)

top_terms <- tidy_lda %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  mutate(term = factor(paste(term, topic, sep = "__"), 
                       levels = rev(paste(term, topic, sep = "__")))) %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  labs(title = "Top 10 terms in each LDA topic",
       x = NULL, y = expression(beta)) +
  facet_wrap(~ topic, ncol = 4, scales = "free") +
        theme(axis.text = element_text(size = 18),
              axis.title = element_text(size = 20, face = "bold"),
              strip.text.x = element_text(size = 22)
              )

```
???
my research could be mapped into one of those topic
---
# Sentiment analysis

```{r SAplot,fig.width=12}
contributions <- wordfabs %>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  group_by(word) %>%
  summarize(occurences = n(),
            contribution = sum(score))

contributions %>%
  top_n(25, abs(contribution)) %>%
  mutate(word = reorder(word, contribution)) %>%
  ggplot(aes(word, contribution, fill = contribution > 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
        theme(axis.text = element_text(size = 18),
              axis.title = element_text(size = 20, face = "bold"),
              )
```

---

# Journal Tones

```{r EI,fig.width=12}
words_by_journal <- wordfabs %>%
  count(journal, word, sort = TRUE) %>%
  ungroup()

tf_idf <- words_by_journal %>%
  bind_tf_idf(word, journal, n) %>%
  arrange(desc(tf_idf))

tf_idf %>%
  group_by(journal) %>%
  top_n(10, tf_idf) %>%
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(word, tf_idf, fill = journal)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ journal, scales = "free") +
  ylab("tf-idf in title") +
  coord_flip()
```

---
# Final Comments

--

## Larger scope

--

## Write papers

--

## Evaluate journals

---
class: inverse, center, middle

# From Yahoo! to Google

???
Yahoo! use manually category. Google use algorithm.
---

class: center, middle

# Thanks!

Slides created via the R package [**xaringan**](https://github.com/yihui/xaringan).

Source Code is [**here**](https://github.com/yufree/presentation/blob/gh-pages/textmining/textmining.Rmd)

Contact me [**here**](https://yufree.cn/en)

---
# Scan

```{r QRcode,fig.height=6}
par(mar=c(0,0,0,0))
image(qrencode_raster("https://yufree.github.io/presentation/textmining/textmining.html"), 
      asp=1, col=c("white", "black"), axes=FALSE, 
      xlab="", ylab="")
```

