---
title: "Text Mining for Academic Journals"
subtitle: "Miao Yu"
author: "Pawliszyn Research Group"
date: "2017/09/20"
output:
  xaringan::moon_reader:
    css: ["default", "my-theme.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.align='center',echo = F, cache=T,message=FALSE,warning=FALSE,comment=NA)
# You need to install those packages by 'install.packages('PACKAGE NAME')' before use them
library(tidyverse)
library(lubridate)
library(stringr)
library(broom)
library(scales)
library(widyr)
library(igraph)
library(ggraph)
library(topicmodels)
library(qrencoder)
library(ngramr)
library(gtrendsR)
```
class: inverse, center, middle

# Papers

???
This presentation is all about papers or research articles published on academic journals

My topic will cover my experiences about using text mining to find hot point in analytical chemistry. I will also cover some usages about text mining for PDF or young scientist.

This slides is actually a template. You could make small modification to find your answer.
---

## How far away between PostDoc and PI?

--
```{r echo=FALSE, out.width='80%'}
knitr::include_graphics('https://yufree.github.io/presentation/figure/pdpi.png')
```

von Bartheld CS, Houmanfar R, Candido A. (2015)
https://doi.org/10.7717/peerj.1262

???
It's not easy to survive in academia, especially when lots of the technique are younger than ourself

---
class: inverse, center, middle

### Genomics/Protomics/Metabolomics

### Quantum Computing

### Big Data

### Nanotechnology

### Artificial Intelligence

### Blockchain

### 3D Printing

### Precision Medicine

### MOOC

???

In the future, more and more new trend or technology will change our life, including our research.

We might have to change our research in the near future by including new technology.

Text mining is one of them which I think almost any researcher could benefit from it.

Let's start with a simple question: how to change research area

---
# Common workflow
--

## 1. Search keywords on WOS/Pubmed/Scopus
--

## 2. Collect papers
--

## 3. Start with few reviews/feature articles
--

## 4. Update with RSS

---
class: inverse, center, middle

# Text Mining

???

keywords have no idea about the whole subject

PDF should have a larger scope as a Prof? no, editor

Text Mining, or natural language processing, use statistical method to find the rules behind literatures. It cover term frequency, term frequency–inverse document frequency, word co-ocurrences, topic modeling, sentiment analysis, etc.

It will help us
---

# Data collection

Here I collected the information from all the papers published in *Analytical Chemistry* in the past five years

```{r fetchdata,message=TRUE,echo=TRUE}
# devtools::install_github('yufree/scifetch')
library(scifetch)
query <- '0003-2700[TA] AND 2012/08:2017/08[DP]'
# fetch data
tmdf <- getpubmed(query, start = 1, end = 10000) %>%
        getpubmedtbl() %>%
        mutate(time = as.POSIXct(date, origin = "1970-01-01"),
         month = round_date(date, "month"))
```

???

five year means the trends currently happened while not included in textbook
we found 8676 records

---
# Frequently used words

```{r papers}
wordft <- tmdf %>%
        filter(nchar(title) > 0) %>%
        unnest_tokens(word, title) %>%
        anti_join(stop_words) %>%
        filter(str_detect(word, "[^\\d]")) %>%
        group_by(word) %>%
        mutate(word_total = n()) %>%
        ungroup() %>%
        mutate(source = 'title')

wordfabs <- tmdf %>%
        filter(nchar(abstract) > 0) %>%
        unnest_tokens(word, abstract) %>%
        anti_join(stop_words) %>%
        filter(str_detect(word, "[^\\d]")) %>%
        group_by(word) %>%
        mutate(word_total = n()) %>%
        ungroup() %>%
        mutate(source = 'abstract')

wordf <- rbind.data.frame(wordft[,c('date','time','month','word','word_total','source','line')],wordfabs[,c('date','time','month','word','word_total','source','line')])

```

```{r Frequently used words,fig.width=8}
wordf %>%
        count(word,word_total,sort = T) %>%
        top_n(10, n) %>%
        mutate(word = reorder(word, n)) %>%
        ggplot(aes(word, n)) +
        geom_col(show.legend = FALSE) +
        ylab("Top 10 frequently used words in title/abstract") +
        coord_flip() +
        theme(axis.text=element_text(size=18),
              axis.title=element_text(size=20,face="bold"))
```

???
Something about mass spectrum and biological analysis

---
# Temporal Trends - Growing Words

```{r time,fig.width=12}
papers_per_month <- tmdf %>%
  group_by(month) %>%
  summarize(month_total = n())

word_month_counts <- wordft %>%
  filter(word_total >= 100) %>%
  count(word, month) %>%
  complete(word, month, fill = list(n = 0)) %>%
  inner_join(papers_per_month, by = "month") %>%
  mutate(percent = n / month_total) %>%
  mutate(year = year(month) + yday(month) / 365) %>%
        filter(percent < 0.8)

mod <- ~ glm(cbind(n, month_total - n) ~ year, ., family = "binomial")

slopes <- word_month_counts %>%
  nest(-word) %>%
  mutate(model = map(data, mod)) %>%
  unnest(map(model, tidy)) %>%
  filter(term == "year") %>%
  arrange(desc(estimate))

slopes %>%
  head(9) %>%
  inner_join(word_month_counts, by = "word") %>%
  mutate(word = reorder(word, -estimate)) %>%
  ggplot(aes(month, n / month_total, color = word)) +
  geom_line(show.legend = FALSE) +
  scale_y_continuous(labels = percent_format()) +
  facet_wrap(~ word, scales = "free_y") +
  expand_limits(y = 0) +
  labs(x = "Year",
       y = "Percentage of titles containing this term",
       title = "9 fastest growing words",
       subtitle = "Judged by growth rate over 5 years")+
        theme(axis.text = element_text(size = 18),
              axis.title = element_text(size = 20, face = "bold"),
              strip.text.x = element_text(size = 22)
              )
```
???
term frequency is count data, so I use binomial regression to compute the slope about time. Here I select 9 fast growing words in the titles. We could find more concepts are actually about living system. Also I found metabolomics which is actually what I did in the past year since I came Waterloo.

---

# Temporal Trends - Shrinking Words
```{r time2,fig.width=12}
slopes %>%
  tail(9) %>%
  inner_join(word_month_counts, by = "word") %>%
  mutate(word = reorder(word, estimate)) %>%
  ggplot(aes(month, n / month_total, color = word)) +
  geom_line(show.legend = FALSE) +
  scale_y_continuous(labels = percent_format()) +
  facet_wrap(~ word, scales = "free_y") +
  expand_limits(y = 0) +
  labs(x = "Year",
       y = "Percentage of titles containing this term",
       title = "9 fastest shrinking words",
       subtitle = "Judged by growth rate over 5 years")+
        theme(axis.text = element_text(size = 18),
              axis.title = element_text(size = 20, face = "bold"),
              strip.text.x = element_text(size = 22)
              )
```

???
It is interesting to find some words which currently get into our textbook are disappearing as hot spots.
---
class: inverse, center, middle

# Solid Phase MicroExtraction

---

# From words to terms

```{r SPME}
library(scifetch)
query <- 'Solid Phase MicroExtraction[MH] AND 2007/08:2017/08[DP]'
tmdf <- getpubmed(query, start = 1, end = 10000) %>%
        getpubmedtbl() %>%
        mutate(time = as.POSIXct(date, origin = "1970-01-01"),
         month = round_date(date, "month"))
```
```{r trend,fig.width=12}
papers_per_month <- tmdf %>%
  group_by(month) %>%
  summarize(month_total = n())

wordfabs <- tmdf %>%
        filter(nchar(abstract) > 0) %>%
        unnest_tokens(word, abstract) %>%
        anti_join(stop_words) %>%
        filter(str_detect(word, "[^\\d]")) %>%
        group_by(word) %>%
        mutate(word_total = n()) %>%
        ungroup() 

title_word_pairs <- wordfabs %>%
        pairwise_count(word,line,sort = TRUE)

set.seed(42)
title_word_pairs %>%
  filter(n >= 800) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 2) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  labs(title = "Bigrams in abstract") +
  theme_void()+
        theme(axis.text = element_text(size = 22)
              )
```
???
I found SPME is a technique with its hard core around extraction. I don't think I should go into this area too much. Instead, I try to find some words related to this technique like mass spectrum.
---

# Topic Model

```{r TM}
query <- '(1095-9203[TA] OR 0028-0836[TA]) AND 2014/08:2017/08[DP]'
xml2_1 <- getpubmed(query, start = 1, end = 10000)
xml2_2 <- getpubmed(query, start = 10001, end = 20000)

tmdf1 <- getpubmedtbl(xml2_1)
tmdf2 <- getpubmedtbl(xml2_2)

tmdf <- bind_rows(tmdf1,tmdf2) %>%
        mutate(time = as.POSIXct(date, origin = "1970-01-01"),
         month = round_date(date, "month"))

wordfabs <- tmdf %>%
        filter(nchar(abstract) > 0) %>%
        unnest_tokens(word, abstract) %>%
        anti_join(stop_words) %>%
        filter(str_detect(word, "[^\\d]")) %>%
        group_by(word) %>%
        mutate(word_total = n()) %>%
        ungroup()

wordft <- tmdf %>%
        filter(nchar(title) > 0) %>%
        unnest_tokens(word, title) %>%
        anti_join(stop_words) %>%
        filter(str_detect(word, "[^\\d]")) %>%
        group_by(word) %>%
        mutate(word_total = n()) %>%
        ungroup()
```
```{r TMplot,fig.width=12}
desc_dtm <- wordfabs %>%
        count(line, word, sort = TRUE) %>%
        ungroup() %>%
        cast_dtm(line, word, n)

desc_lda <- LDA(desc_dtm, k = 8, control = list(seed = 42))
tidy_lda <- tidy(desc_lda)

top_terms <- tidy_lda %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  mutate(term = factor(paste(term, topic, sep = "__"), 
                       levels = rev(paste(term, topic, sep = "__")))) %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  labs(title = "Top 10 terms in each LDA topic",
       x = NULL, y = expression(beta)) +
  facet_wrap(~ topic, ncol = 4, scales = "free") +
        theme(axis.text = element_text(size = 18),
              axis.title = element_text(size = 20, face = "bold"),
              strip.text.x = element_text(size = 22)
              )

```
???
Topic modeling use latent Dirichlet allocation to find potential topic among the papers.
Climate change, quantum, tumour, protein, neurons, evolution and astronomy are what they like.
my research should be mapped into one of those topic.

---

# Insights in Text Mining

## Mass Spectrum 

## Bioanalytical Chemistry

## Solid Phase MicroExtraction

## Living Samples
--

Use Mass Spectrum and Solid Phase MicroExtraction to study Omics by **in vivo** sampling

---
class: inverse, center, middle

# Metabolomics

---
# Sentiment analysis

```{r SAplot,fig.width=12}
contributions <- wordfabs %>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  group_by(word) %>%
  summarize(occurences = n(),
            contribution = sum(score))

contributions %>%
  top_n(25, abs(contribution)) %>%
  mutate(word = reorder(word, contribution)) %>%
  ggplot(aes(word, contribution, fill = contribution > 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
        theme(axis.text = element_text(size = 18),
              axis.title = element_text(size = 20, face = "bold"),
              )
```
???
Should we use neutral word? Sentiment analysis could show this.
---

# Journal Tones

```{r EI,fig.width=12}
words_by_journal <- wordft %>%
  count(journal, word, sort = TRUE) %>%
  ungroup()

tf_idf <- words_by_journal %>%
  bind_tf_idf(word, journal, n) %>%
  arrange(desc(tf_idf))

tf_idf %>%
  group_by(journal) %>%
  top_n(10, tf_idf) %>%
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(word, tf_idf, fill = journal)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ journal, scales = "free") +
  ylab("tf-idf in title") +
  coord_flip()+
        theme(axis.text = element_text(size = 18),
              axis.title = element_text(size = 20, face = "bold"),
              strip.text.x = element_text(size = 22)
              )
```
???
Here I use term frequency–inverse document frequency, which could show the preferred words in different parts of records.
Science has American accent and Nature has British tones

---
# Word usage

```{r data,fig.width=12,fig.height=3}
ng  <- ngram(c("data is", "data are"), year_start = 1950)
ggplot(ng, aes(x=Year, y=Frequency, colour=Phrase)) +
  geom_line() +
        ggtitle("Usage in books") +
        scale_color_manual(values=c('red', 'blue'))

lang_trend <- gtrends(c("data is", "data are"))
df <- lang_trend$interest_over_time
ggplot(df, aes_string(x = "date", y = "hits", color = "keyword")) +
    geom_line() +
    xlab("Year") +
    ylab("Search hits") +
    ggtitle("Usage in keywords search") +
        scale_color_manual(name = "Phrase",values=c('blue', 'red'))+
        guides(col = guide_legend(reverse = TRUE))
```
???
You might also use text mining to help you write papers.
---
# Final Comments

--

## Larger scope

--

## Capture trends

--

## Write papers

---
class: inverse, center, middle

# From Yahoo! to Google

???
Yahoo! use manually category. Google use algorithm.
---

class: center, middle

# Thanks!

Slides created via the R package [**xaringan**](https://github.com/yihui/xaringan).

Source Code is [**here**](https://github.com/yufree/presentation/blob/gh-pages/textmining/textmining.Rmd)

Contact me [**here**](https://yufree.cn/en) or @yu_free

???
Online textbook about textmining: http://tidytextmining.com/
---
# Scan

```{r QRcode,fig.height=6}
par(mar=c(0,0,0,0))
image(qrencode_raster("https://yufree.github.io/presentation/textmining/textmining.html"), 
      asp=1, col=c("white", "black"), axes=FALSE, 
      xlab="", ylab="")
```
